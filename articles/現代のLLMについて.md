# 現代のLLMについて〜Attention is all you need!〜

LLMとは、Large Language Modelの略で日本語では大規模言語モデルと言われます。具体的にはchatGPTやGeminiやClaudeなどに代表されるAIのことを言います。
最近突然LLMは登場しものすごい勢いで進化しています。その裏には*Attention*機構と呼ばれるAIモデルにおける革新的なアイデアの登場があります。

この記事では前提知識として一般的なAIの学習の方法である勾配降下法から始まり具体的なイメージを掴んでもらうためにニューラルネットワークを説明します。その後いよいよLLMの内部構造・Attention機構とはつまるところ何なのかを説明していきます。それらを踏まえて最後にAIについての少し哲学的な自論を話します。

数学的な知識がなくても現代のブラックボックスの筆頭であるAIを感覚的に理解してもらえるよう作っておりますので、最後まで読んでくれたら嬉しいです。

## AIとは何？
AIは*Artificial Intelligence*の略で日本語で言うと*人工知能*と言う意味です。その名の通り人工的に作った知能のことを指します。

一般的にはAIは人工的に作られた知能のことを指すことが多いですが、個人的には人間がプログラムしたコード自体が動くのではなくコードによって生成されたコードが動くというイメージがAIの定義です。
どういうことかというと、学習のステップが存在すると言うことです。

## 学習＝勾配降下法＝*Parameter*探しの旅

例えば具体的に質問に対して自然な回答をするようなAIを考えてみましょう。形式的にこのAIを質問という入力を受け取って回答という出力をするような便宜的な*関数*を考えてみます。
```math
f: 質問 -> 回答
```
AI開発者の目標はこの関数を作ることもしくはこの関数に限りなく近い別の関数を作ることです。

外から見ると、この関数*f*はとんでもなく複雑でそれは人間の脳を再現しているように見えます。そのような関数を実際に人の手で書いていくことはほとんど不可能です。
なので数学の具体的な数値の計算を用いてこの関数*f*のとりうる状態を表現します。例えば以下のように。
```math
f(input) = input * Parameter
```
このとき`input`や`output`というのは単なる数ではなく数がたくさん集まったものと考えてください。(具体的にはベクトルとなります)
AI開発者の目標はいい感じのParameterというデータを作ることに変わりました。
> 注：`*`というのはこのたくさんの数字のグループに定義される何らかの演算です。(具体的には行列計算の組み合わさった非常に複雑なもの)

AI開発者の目標はいい感じの関数*f*が**たまたま**できるParameterを作ることです。この「いい感じ」というのを厳密に定義します。
結局何がしたかったかというと「任意の質問を受け取って自然な回答を返すような関数」を作ることでした。つまりここでの「いい感じ」とは自然な回答を返すという意味です。コンピュータのためにそのための絶対的に正しい答えを作ります。例えばこんな感じ。
```
質問：人間は何本足ですか？
回答：人間は2本足です。
```
この正解からどれだけずれているかを数値的に表してそれが小さい時に「いい感じ」であるとしましょう。
ちなみにこの正解はインターネットから大量に自動生成します。

ここで「*Parameter*を受け取ってそれによってできた関数*f*が理想からのズレをどれぐらい持つかを返す関数」を考えます。これを損失関数と呼びます。
```
損失関数：Parameter -> 理想からのズレ
```
AI開発者の目標はこの損失関数の最小値を求めることに変わりました。
さて関数の最小値を求める方法は高校では微分です。というわけで損失関数を微分します。

微分の定義は、接線の傾きですから。
```
f'(x) -> ( f(x + h) - f(x) )/h		( h -> 0)
```
となります。流石にこの非常に複雑だと思われる損失関数を直接解析的に微分するのはハードなので数値微分というものを使います。
```
f'(x) ≒ ( f(x + h) - f(x) )/h （hは0に近い数）
```
さっきは`lim`を使って限りなく近づけていましたが、大体0の数(10^-10)みたいなものを使って微分結果を近似します。

損失関数がある程度滑らかなものであったならば、損失関数の微分係数の方向に移動してそこでまた微分係数を計算し...とすればいつか最小値に辿り着きそうです。

数学の言葉を使わずに説明するなら、「少し*Parameter*をいじってみてモデルの精度がどう変わるかを見ることによってどちらの*方向*に*Parameter*をいじれば精度が良くなるかを特定する。」ということを繰り返しやっています。

損失関数という凸凹な地面を持つ広大な土地を目隠しの人が歩き回って一番深い谷を見つけるイメージです。この時微分というのはその人の足が捉える地面の傾きに相当します。

ただし、同じスピードで歩くとかなり時間がかかってしまうので、傾きが大きければ大きいほど大きく動くことにします。
```
Parameter_k+1 = 勾配ベクトル * 学習率 + Parameter_k
```
>注：勾配ベクトルは各変数における偏微分を集めたものです）
>注：勾配ベクトルは数学的には最も上に登る方向を指すので、マイナスをつけておく必要がありますが本質ではないので気にしないでOKです。

学習率はどれぐらい大胆に動くかを表します。
大抵の場合最初は学習率は大きめに設定してその後小さくしていくと良いので、この*Parameter*探しの旅がどれぐらい進んだかで自動で学習率を動的に変化させていくこともあります。

長々と説明してきましたが、この*Parameter*探しの旅がまさに学習という工程です。

まとめると、AIにおける学習とは「正解からの距離に基づきパラメータ空間に一つの軸を作り、傾きに従って少しずつパラメータを変更していくことで、正解からの距離が0にちかいパラメータを探し当てる」ということです。

## ニューラルネットワーク
さっきはどんなふうに学習を進めるのかを説明していましたが、どんなふうにパラメータを以て計算をするのかについては全く触れてきませんでした。このどんなふうに計算するかにはたくさん種類が存在します。どんなAIを作るかによって最適な計算方法は変わってきますが、一番有名なのはニューラルネットワークでLLMにも使われていますのでニューラルネットワークに絞って具体的な説明をします。

ニューラルネットワークは、巨大な行列でできたパラメータを掛け算していくという計算方法です。
```
Output = Input * Param_1 * Papram_2 * Param_3 * ... * Param_n
```
この行列の演算を細く分解して四則演算レベルまで持っていくと、まるで人間の脳の中にあるニューロン細胞のつながりのように見えてくるというわけでニューラルネットワークと呼ばれています。
>注：ネットワークにはインターネット的な意味は含まれず網目のように繋がっているという意味です。

具体的には重みつき和を複数のノードで同時に計算していくようなものになっていますが、あまり面白くないので取り上げません。

損失関数の計算にはRMSEが使われることが多いです。また毎度のノードを非線形な関数でフィルターすることで「ズレ」を作り出し微分で最適な変更方向を見つけやすいようになっています。これによってそれ自体では線形なってしまうニューラルネットワークに非線形な要素が加わりその分の情報をモデルが中に蓄えることができるようになります。具体的にはシグモイド関数などが使われます。

## Transformer
ここではLLMに実際に組み込まれている概念を説明していきます。

Transformerを解説する前に、Transformer以前の自然言語処理の技術について解説していきます。自然言語処理とは、英語・日本語など人間が使う言葉を機械を持って調べるという分野です。

LLMはこれまでの文字を見てその直後に続く言葉を推論します。それを何度も繰り返すことで文章を作っていきます。イメージはスマホなどの予測変換のレベルの高い版のようなものです。一番最初に来た予測変換を連打していくとそれっぽい文章ができます。（試してみるとわかりますが大抵絵文字で収束します。）

LLMは文字を文字として捉えるのではなくトークンという単位で管理します。英語で言うと1wordよりも少しミクロな接頭辞などを分けて考える単位です。


<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE1NDQyODYyMzksNDAxMjI2NTE3XX0=
-->