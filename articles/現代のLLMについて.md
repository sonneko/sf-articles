# 現代のLLMについて〜*Attention Is All You Need!*〜

LLMとは、Large Language Modelの略で日本語では大規模言語モデルと言われます。具体的にはchatGPTやGeminiやClaudeなどに代表されるAIのことを言います。
最近突然LLMは登場しものすごい勢いで進化しています。その裏には*Attention*機構と呼ばれるAIモデルにおける革新的なアイデアの登場があります。

この記事では前提知識として一般的なAIの学習の方法である勾配降下法から始まり具体的なイメージを掴んでもらうためにニューラルネットワークを説明します。その後いよいよLLMの内部構造・Attention機構とはつまるところ何なのかを説明していきます。それらを踏まえて最後にAIについての少し哲学的な自論を話します。

数学的な知識がなくても現代のブラックボックスの筆頭であるAIを感覚的に理解してもらえるよう作っておりますので、最後まで読んでくれたら嬉しいです。

## AIとは何？
AIは*Artificial Intelligence*の略で日本語で言うと*人工知能*と言う意味です。その名の通り人工的に作った知能のことを指します。

一般的にはAIは人工的に作られた知能のことを指すことが多いですが、個人的には人間がプログラムしたコード自体が動くのではなくコードによって生成されたコードが動くというイメージがAIの定義です。
どういうことかというと、学習のステップが存在すると言うことです。

## 学習＝勾配降下法＝*Parameter*探しの旅

例えば具体的に質問に対して自然な回答をするようなAIを考えてみましょう。形式的にこのAIを質問という入力を受け取って回答という出力をするような便宜的な*関数*を考えてみます。
```math
f: 質問 -> 回答
```
AI開発者の目標はこの関数を作ることもしくはこの関数に限りなく近い別の関数を作ることです。

外から見ると、この関数*f*はとんでもなく複雑でそれは人間の脳を再現しているように見えます。そのような関数を実際に人の手で書いていくことはほとんど不可能です。
なので数学の具体的な数値の計算を用いてこの関数*f*のとりうる状態を表現します。例えば以下のように。
```math
f(input) = input * Parameter
```
このとき`input`や`output`というのは単なる数ではなく数がたくさん集まったものと考えてください。(具体的にはベクトルとなります)
AI開発者の目標はいい感じのParameterというデータを作ることに変わりました。
> 注：`*`というのはこのたくさんの数字のグループに定義される何らかの演算です。(具体的には行列計算の組み合わさった非常に複雑なもの)

AI開発者の目標はいい感じの関数*f*が**たまたま**できるParameterを作ることです。この「いい感じ」というのを厳密に定義します。
結局何がしたかったかというと「任意の質問を受け取って自然な回答を返すような関数」を作ることでした。つまりここでの「いい感じ」とは自然な回答を返すという意味です。コンピュータのためにそのための絶対的に正しい答えを作ります。例えばこんな感じ。
```
質問：人間は何本足ですか？
回答：人間は2本足です。
```
この正解からどれだけずれているかを数値的に表してそれが小さい時に「いい感じ」であるとしましょう。
ちなみにこの正解はインターネットから大量に自動生成します。

ここで「*Parameter*を受け取ってそれによってできた関数*f*が理想からのズレをどれぐらい持つかを返す関数」を考えます。これを損失関数と呼びます。
```
損失関数：Parameter -> 理想からのズレ
```
AI開発者の目標はこの損失関数の最小値を求めることに変わりました。
さて関数の最小値を求める方法は高校では微分です。というわけで損失関数を微分します。

微分の定義は、接線の傾きですから。
```
f'(x) -> ( f(x + h) - f(x) )/h		( h -> 0)
```
となります。流石にこの非常に複雑だと思われる損失関数を直接解析的に微分するのはハードなので数値微分というものを使います。
```
f'(x) ≒ ( f(x + h) - f(x) )/h （hは0に近い数）
```
さっきは`lim`を使って限りなく近づけていましたが、大体0の数(10^-10)みたいなものを使って微分結果を近似します。

>注：これは順伝播方法と呼ばれる方法で、実際にはより効率が良い逆伝播法と呼ばれる合成関数の微分を利用した計算方法が使われています。ここではわかりやすさのため順伝播法のみを説明しますが、逆伝播法も微分をするという意味では全く同じです。

損失関数がある程度滑らかなものであったならば、損失関数の微分係数の方向に移動してそこでまた微分係数を計算し...とすればいつか最小値に辿り着きそうです。

数学の言葉を使わずに説明するなら、「少し*Parameter*をいじってみてモデルの精度がどう変わるかを見ることによってどちらの*方向*に*Parameter*をいじれば精度が良くなるかを特定する。」ということを繰り返しやっています。

損失関数という凸凹な地面を持つ広大な土地を目隠しの人が歩き回って一番深い谷を見つけるイメージです。この時微分というのはその人の足が捉える地面の傾きに相当します。

ただし、同じスピードで歩くとかなり時間がかかってしまうので、傾きが大きければ大きいほど大きく動くことにします。
```
Parameter_k+1 = 勾配ベクトル * 学習率 + Parameter_k
```
>注：勾配ベクトルは各変数における偏微分を集めたものです）
>注：勾配ベクトルは数学的には最も上に登る方向を指すので、マイナスをつけておく必要がありますが本質ではないので気にしないでOKです。

学習率はどれぐらい大胆に動くかを表します。
大抵の場合最初は学習率は大きめに設定してその後小さくしていくと良いので、この*Parameter*探しの旅がどれぐらい進んだかで自動で学習率を動的に変化させていくこともあります。

長々と説明してきましたが、この*Parameter*探しの旅がまさに学習という工程です。

まとめると、AIにおける学習とは「正解からの距離に基づきパラメータ空間に一つの軸を作り、傾きに従って少しずつパラメータを変更していくことで、正解からの距離が0にちかいパラメータを探し当てる」ということです。

## ニューラルネットワーク
さっきはどんなふうに学習を進めるのかを説明していましたが、どんなふうにパラメータを以て計算をするのかについては全く触れてきませんでした。このどんなふうに計算するかにはたくさん種類が存在します。どんなAIを作るかによって最適な計算方法は変わってきますが、一番有名なのはニューラルネットワークでLLMにも使われていますのでニューラルネットワークに絞って具体的な説明をします。

ニューラルネットワークは、巨大な行列でできたパラメータを掛け算していくという計算方法です。
```
Output = Input * Param_1 * Papram_2 * Param_3 * ... * Param_n
```
この行列の演算を細く分解して四則演算レベルまで持っていくと、まるで人間の脳の中にあるニューロン細胞のつながりのように見えてくるというわけでニューラルネットワークと呼ばれています。
>注：ネットワークにはインターネット的な意味は含まれず網目のように繋がっているという意味です。

具体的には重みつき和を複数のノードで同時に計算していくようなものになっていますが、あまり面白くないので取り上げません。

損失関数の計算にはRMSEが使われることが多いです。また毎度のノードを非線形な関数でフィルターすることで「ズレ」を作り出し微分で最適な変更方向を見つけやすいようになっています。これによってそれ自体では線形なってしまうニューラルネットワークに非線形な要素が加わりその分の情報をモデルが中に蓄えることができるようになります。具体的にはシグモイド関数などが使われます。

## Transformer
ここではLLMに実際に組み込まれている概念を説明していきます。

LLMはこれまでの文字を見てその直後に続く言葉を推論します。それを何度も繰り返すことで文章を作っていきます。イメージはスマホなどの予測変換のレベルの高い版のようなものです。一番最初に来た予測変換を連打していくとそれっぽい文章ができます。（試してみるとわかりますが大抵絵文字で収束します。）

```
昨日は雨だったけど、今日..
→「は」を予想する
昨日は雨だったけど、今日は..
→「晴れ」を予想する
昨日は晴れだったけど今日は晴れ..
→「だ」を予想
昨日は晴れだったけど今日は晴れだ..
→「った」を予想
昨日は晴れだったけど今日は晴れだった
→「。」を予想
昨日は晴れだったけど今日は晴れだった。
```

LLMは文字を文字として捉えるのではなくトークンという単位で管理します。英語で言うと1wordよりも少しミクロな接頭辞などを分けて考えるぐらいの単位です。

それぞれのトークンをベクトルとして管理します。ベクトルとはここでは方向のことではなく数の集まりのことです。例えば`[0, 0, 0, 0]`などです。数の個数は場合によって異なります。

LLMはこれまでのトークン列を使って次のトークンを予測します。このときそれぞれのトークン同士の間にある関係を計算しそれを積算していきます。トークンが他のトークンにその意味を与えていきただのトークンの意味ではなくより文脈を考慮した深い意味を持ったベクトルに変換されていきます。

例えば、「コーヒー」というトークンが「ミルク」や「混ぜる」といったトークンの意味を受けて「カフェオレ」という意味を持つベクトルに近くなっていくといったイメージです。人間もトークンを一つ見ても何のことだか分からなくても周りの文脈で意味を想像することができますよね。大体そんな感じです。

それは具体的には次のような計算として行われます。

例えば、もしこんなトークン列があって`Token_5`を予測するときには..
```
Token_1 Token_2 Token_3 Token_4
```
まずはそれぞれに対応するベクトルに変換されます。
```
[...] [...] [...] [...]
```

それぞれのトークンのベクトルに対して`Query`行列を用いて同じように「どんな情報が欲しいか」を表す`Query`ベクトルを作ります。

同様に`Value`行列を用いて「どんな意味を他のトークンにもたらせば良いか」を表す`Value`ベクトルを作ります。

`Key`行列を用いて「どのトークンから意味を受け取れば良いか」を表す`Key`ベクトルを作ります。

まとめると`Key`を目印に`Query`がはたらきそれに対して`Value`が送られるということです。

`Key`と`Query`が*似ている*ベクトルであればあるほど`Value`がターゲットのトークンのベクトルに対して加算されます。トークンの数分`Key`・`Query`・`Value`ベクトルがあるということなので、実際には加算ではなく平均値の足し算になります。

...という操作を行った後、ベクトルをニューラルネットワークに通すというのを1セットとして、何度もこのセットが繰り返されます。
これが*Attention機構*です。実はサブタイトルの*Attention Is All You Need*の元ネタはこの計算機構を世界で初めて提案した有名な論文のタイトルです。

[元ネタの論文](https://arxiv.org/pdf/1706.03762)

>ちなみにですがこの論文のヘッダを見るとわかりますが、この論文は*Google*のAI研究者によるものです。
商業サービスという面では*OpenAI*による*chatGPT*に抜かれていますが、技術的には*Google*が先なんです。~~*Google*恐ろしや~~

これでアルゴリズムの直感的な説明は終わりなのですが、これで本当にあんなAIができてしまうのかと驚嘆した方が多いのではないかと思います。そんな方のためにこの計算がどれほど大変か、そして我々の脳がどれほどすごいかを説明します。

先の*Attention機構*の説明では、色々なところにパラメータが登場しました。
- トークンに対応するベクトル
- `Key`行列
- `Query`行列
- `Value`行列
- ニューラルネットワークのパラメータ

これらのパラメータの数の合計は、現代のモデルでは数千億個あります。それらのパラメータを調節するには、数千億次元の遥かなる海を漂う必要があるというわけです。その果てしない学習のステップでは数千台の専用の高性能な電子チップが数カ月動き続ける必要があります。消費電力は街一つ分と言われています。冷却水の供給のための電力も含めると
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTI1MjgyNjI3OCwxMjMzMjM0OTYwLC0xMz
Y2MjY5MjY5LC0xNTQ0Mjg2MjM5LDQwMTIyNjUxN119
-->