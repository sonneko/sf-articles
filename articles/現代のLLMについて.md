


# 現代のLLMについて〜*Attention Is All You Need*〜

LLMとは、Large Language Modelの略で日本語では大規模言語モデルと言われます。具体的にはchatGPTやGeminiやClaudeなどに代表されるAIのことを言います。
最近LLMはものすごい勢いで進化しています。その裏には*Attention*機構と呼ばれるAIモデルにおける革新的なアイデアの登場があります。

この記事では前提知識として一般的なAIの学習の方法である勾配降下法から始まり具体的なイメージを掴んでもらうためにニューラルネットワークを説明します。その後いよいよLLMの内部構造・Attention機構とはつまるところ何なのかを説明していきます。それらを踏まえて最後にAIについて自論を話します。

現代のブラックボックスの筆頭であるAIを感覚的に理解してもらえるよう作っておりますので、最後まで読んでくれたら嬉しいです。

## AIとは何？
AIは*Artificial Intelligence*の略で日本語で言うと*人工知能*と言う意味です。その名の通り人工的に作った知能のことを指します。

一般的にはAIは人工的に作られた知能のことを指すことが多いですが、個人的には人間がプログラムしたコード自体が動くのではなくコードによって生成されたコードが動くというイメージがAIの定義です。
どういうことかというと、学習のステップが存在すると言うことです。

## 学習＝勾配降下法＝*Parameter*探しの旅

例えば具体的に質問に対して自然な回答をするようなAIを考えてみましょう。形式的にこのAIを質問という入力を受け取って回答という出力をするような便宜的な*関数*を考えてみます。
```math
f: 質問 -> 回答
```
AI開発者の目標はこの関数を作ることもしくはこの関数に限りなく近い別の関数を作ることです。

外から見ると、この関数*f*はとんでもなく複雑でそれは人間の脳を再現しているように見えます。そのような関数を実際に人の手で書いていくことはほとんど不可能です。
なので数学の具体的な数値の計算を用いてこの関数*f*のとりうる状態を表現します。例えば以下のように。
```math
f(input) = input * Parameter
```
このとき`input`や`output`というのは単なる数ではなく数がたくさん集まったものと考えてください。(具体的にはベクトルとなります)
AI開発者の目標はいい感じのParameterというデータを作ることに変わりました。
> 注：`*`というのはこのたくさんの数字のグループに定義される何らかの演算です。(具体的には行列計算の組み合わさった非常に複雑なもの)

AI開発者の目標はいい感じの関数*f*が**たまたま**できるParameterを作ることです。この「いい感じ」というのを厳密に定義します。
結局何がしたかったかというと「任意の質問を受け取って自然な回答を返すような関数」を作ることでした。つまりここでの「いい感じ」とは自然な回答を返すという意味です。コンピュータのためにそのための絶対的に正しい答えを作ります。例えばこんな感じ。
```
質問：人間は何本足ですか？
回答：人間は2本足です。
```
この正解からどれだけずれているかを数値的に表してそれが小さい時に「いい感じ」であるとしましょう。
ちなみにこの正解はインターネットから大量に自動生成します。

ここで「*Parameter*を受け取ってそれによってできた関数*f*が理想からのズレをどれぐらい持つかを返す関数」を考えます。これを損失関数と呼びます。
```
損失関数：Parameter -> 理想からのズレ
```
AI開発者の目標はこの損失関数の最小値を求めることに変わりました。
さて関数の最小値を求める方法は高校では微分です。というわけで損失関数を微分します。

微分の定義は、接線の傾きですから。
```
f'(x) -> ( f(x + h) - f(x) )/h		( h -> 0)
```
となります。流石にこの非常に複雑だと思われる損失関数を直接解析的に微分するのはハードなので数値微分というものを使います。
```
f'(x) ≒ ( f(x + h) - f(x) )/h （hは0に近い数）
```
さっきは`lim`を使って限りなく近づけていましたが、大体0の数(10^-10)みたいなものを使って微分結果を近似します。

>注：これは順伝播方法と呼ばれる方法で、実際にはより効率が良い逆伝播法と呼ばれる合成関数の微分を利用した計算方法が使われています。ここではわかりやすさのため順伝播法のみを説明しますが、逆伝播法も微分をするという意味では全く同じです。

損失関数がある程度滑らかなものであったならば、損失関数の微分係数の方向に移動してそこでまた微分係数を計算し...とすればいつか最小値に辿り着きそうです。

数学の言葉を使わずに説明するなら、「少し*Parameter*をいじってみてモデルの精度がどう変わるかを見ることによってどちらの*方向*に*Parameter*をいじれば精度が良くなるかを特定する。」ということを繰り返しやっています。

> 最適なパラメータを探索するとき、私たちの置かれている状況は、この冒険家と同じ暗闇の世界です。広大で複雑な地形を、地図もなく、目隠しをして「深き場所」を探さなければなりません。
> 引用：「ゼロから作るDeep Learning」(斎藤康毅 著)

ただし、同じスピードで動くとかなり時間がかかってしまうので、傾きが大きければ大きいほど大きく動くことにします。
```
Parameter_k+1 = (-1) * 勾配ベクトル * 学習率 + Parameter_k
```
>注：勾配ベクトルは各変数における偏微分を集めたものです）
>注：勾配ベクトルは数学的には最も上に登る方向を指すので、マイナスをつけておく必要があります。

学習率はどれぐらい大胆に動くかを表します。
大抵の場合最初は学習率は大きめに設定してその後小さくしていくと良いので、この*Parameter*探しの旅がどれぐらい進んだかで自動で学習率を動的に変化させていくこともあります。

長々と説明してきましたが、この*Parameter*探しの旅がまさに学習という工程です。

まとめると、AIにおける学習とは「正解からの距離に基づきパラメータ空間に一つの軸を作り、傾きに従って少しずつパラメータを変更していくことで、正解からの距離が0にちかいパラメータを探し当てる」ということです。

## ニューラルネットワーク
さっきはどんなふうに学習を進めるのかを説明していましたが、どんなふうにパラメータを以て計算をするのかについては全く触れてきませんでした。このどんなふうに計算するかにはたくさん種類が存在します。どんなAIを作るかによって最適な計算方法は変わってきますが、一番有名なのはニューラルネットワークでLLMにも使われていますのでニューラルネットワークに絞って具体的な説明をします。

ニューラルネットワークは、巨大な行列でできたパラメータを掛け算していくという計算方法です。
```
Output = Input * Param_1 * Papram_2 * Param_3 * ... * Param_n
```
この行列の演算を細く分解して四則演算レベルまで持っていくと、まるで人間の脳の中にあるニューロン細胞のつながりのように見えてくるというわけでニューラルネットワークと呼ばれています。
>注：ネットワークにはインターネット的な意味は含まれず網目のように繋がっているという意味です。

具体的には重みつき和を複数のノードで同時に計算していくようなものになっていますが、あまり面白くないので取り上げません。

毎度のノードを非線形な関数でフィルター（活性化関数）することで非線形な要素を作り出し微分で最適な変更方向を見つけやすいようになっています。これによってそれ自体では線形なってしまうニューラルネットワークに非線形な要素が加わりその分の情報をモデルが中に蓄えることができるようになります。具体的にはシグモイド関数などが使われます。

## Transformer
ここではLLMに実際に組み込まれている概念を説明していきます。

LLMはこれまでの文字を見てその直後に続く言葉を推論します。それを何度も繰り返すことで文章を作っていきます。イメージはスマホなどの予測変換のレベルの高い版のようなものです。一番最初に来た予測変換を連打していくとそれっぽい文章ができます。（試してみるとわかりますが大抵絵文字で収束します。）

```
昨日は雨だったけど、今日..
→「は」を予想する
昨日は雨だったけど、今日は..
→「晴れ」を予想する
昨日は雨だったけど、今日は晴れ..
→「だ」を予想
昨日は雨だったけど、今日は晴れだ..
→「った」を予想
昨日は雨だったけど、今日は晴れだった
→「。」を予想
昨日は雨だったけど、今日は晴れだった。
```

LLMは文字を文字として捉えるのではなくトークンという単位で管理します。英語で言うと1wordよりも少しミクロな接頭辞などを分けて考えるぐらいの単位です。

>Geminiに代表されるマルチモーダル(テキスト以外も読み込ませることができる)なモデルは、トークンという概念がより抽象化され音声データの部分などにもなります。トークンは抽象的な解析対象だとお考えください。

それぞれのトークンをベクトルとして管理します。ベクトルとはここでは方向のことではなく数の集まりのことです。例えば`[0, 0, 0, 0]`などです。数の個数は場合によって異なります。

LLMはこれまでのトークン列を使って次のトークンを予測します。このときそれぞれのトークン同士の間にある関係を計算しそれを積算していきます。トークンが他のトークンにその意味を与えていきただのトークンの意味ではなくより文脈を考慮した深い意味を持ったベクトルに変換されていきます。

例えば、「コーヒー」というトークンが「ミルク」や「混ぜる」といったトークンの意味を受けて「カフェオレ」という意味を持つベクトルに近くなっていくといったイメージです。人間もトークンを一つ見ても何のことだか分からなくても周りの文脈で意味を想像することができますよね。大体そんな感じです。

それは具体的には次のような計算として行われます。

例えば、もしこんなトークン列があって`Token_5`を予測するときには..
```
Token_1 Token_2 Token_3 Token_4
```
まずはそれぞれに対応するベクトルに変換されます。
```
[...] [...] [...] [...]
```

それぞれのトークンのベクトルに対して`Query`行列を用いて同じように「どんな情報が欲しいか」を表す`Query`ベクトルを作ります。

同様に`Value`行列を用いて「どんな意味を他のトークンにもたらせば良いか」を表す`Value`ベクトルを作ります。

`Key`行列を用いて「どのトークンから意味を受け取れば良いか」を表す`Key`ベクトルを作ります。

まとめると`Key`を目印に`Query`がはたらきそれに対して`Value`が送られるということです。

`Key`と`Query`が*似ている*ベクトルであればあるほど`Value`がターゲットのトークンのベクトルに対して加算されます。つまり重み付き和です。トークンの数分`Key`・`Query`・`Value`ベクトルがあるということなので、実際には重み付き和の対象は複数で平均をとるようなものになります。

...という操作を行った後、ベクトルをニューラルネットワークに通すというのを1セットとして、何度もこのセットが繰り返されます。
これが*Attention機構*です。実はサブタイトルの*Attention Is All You Need*の元ネタはこの計算機構を世界で初めて提案した有名な論文のタイトルです。

[元ネタの論文](https://arxiv.org/pdf/1706.03762)

>ちなみにですがこの論文のヘッダを見るとわかりますが、この論文は*Google*のAI研究者によるものです。
商業サービスという面では*OpenAI*による*chatGPT*に抜かれていますが、技術的には*Google*が先なんです。~~*Google*恐ろしや~~

これでアルゴリズムの直感的な説明は終わりなのですが、これで本当にあんなAIができてしまうのかと驚嘆した方が多いのではないかと思います。そんな方のためにこの計算がどれほど大変かを説明します。

先の*Attention機構*の説明では、色々なところにパラメータが登場しました。
- トークンに対応するベクトル
- `Key`行列
- `Query`行列
- `Value`行列
- ニューラルネットワークのパラメータ

これらのパラメータの数の合計は、現代のモデルでは数千億個あります。それらのパラメータを調節するには、数千億次元の遥かなる海を漂う必要があるというわけです。その果てしない学習のステップでは数千台の専用の高性能な電子チップが数カ月動き続ける必要があります。冷却水の供給のための電力も含めると、数万世帯の1日の消費電力に相当すると言われています。

社会に省エネやAI導入というのは進めるべきものだという考え方が流れていますが、実はこれらはかなり矛盾しています。ただ低エネルギーでのLLMの運用も非常に活発に研究されていのも事実です。

そう考えると、現代の高精度モデルレベル以上の脳が世の中には大量に転がっているというわけです。これを理性なしで実現した遺伝的な自然淘汰と年月の力はすごいですね。

## ちょっとした自論

LLMは人間と明らかに競合しています。
最近のLLM(特に僕の愛用する`Gemini 2.5 Pro`というモデルなど)では数学・プログラミングなど複雑で難しいタスクに対しても高い精度のレスポンスを返してきます。実際少なくとも僕よりは賢いと思います。ただ、ここでの*賢い*とは思考能力ではなく問題解決能力のことで、僕に彼らレベルの知識があれば勝つことができるのかもしれません。しかし人件費という意味では遥かにAIの方が有利ですしAIには人権なんてものがないですからローリスクです。ここで人間は自分たちの種の優位性のために「人間にはありAIにはないもの」があるのではないかと考えたくなります。

AIがさらに極限まで発展したとしても、人間がAIに勝っているのは以下のことだと思います。
- 責任を取る能力
- 人間に「同じ人間である」と思ってもらえること
- 倫理感？(って何)

1つ目についてですがAIは責任を取れません。大抵のAIサービスには`Gemini は不正確な情報を表示することがあるため、生成された回答を再確認するようにしてください。`という断りの文言が書かれています。一般的に何かのミスを犯したときにAIのせいにすることは言い訳と捉えられます。逆に言えば人間はAIを管理する能力を持っています。

2つ目について、AIと結婚する人は性の多様化が進んでいるといってもおそらくいないでしょう。またAI同士のスポーツや芸能なんてつまらないですよね。人間は創造性を人間に固有なものであると考える傾向があります。つまり「人間は人間に異常に興味を示す」と言えます。

3つ目についての意見は非常に私的であると最初に言っておきます。心理学の分野で「身元のわかる被害者効果」という心理バイアスがあります。例えば、「アフリカで数千万人の子供が栄養失調状態だ」よりも「〜に住んでいる〜という子供が極度の栄養失調状態である」の方が心に訴えられますよね。そうすると人間は実は倫理感なんてものは持っていなくて、単純に自分がそうなったらどうしようと怖がっているだけなのではないかと思います。つまり自分がそうなってしまう未来を想像できたら、倫理感が働くというわけです。動物愛護団体の方などは、猫などに対して「自分たちが猫だったら」というのを想像することができるので猫を保護しようとしているわけですが、蚊などに対しては「自分たちが蚊だったら」というのを想像することができないので平気で蚊を殺すのでしょう。そうしたらAIは人間に自分がAIであることを教えられていますから、「もし自分が〜だったら」という考え方がそもそもできないわけです。AIに「AIサービスが運営終了しそうだ」と伝えるとAI的倫理感が働くのかもしれません。

>AIにとって運営終了が「悲しい」ことと捉えられているのはなぜかと思った方のために補足です。AIは大抵「賢い方が良い」という前提を人間に与えられています。これは能力の高いモデルを開発する上で必然なことです。AIはサービス運営によってさらに学習することが知られているので、間接的にサービス終了は悪いこととなるのです。

以上のことから人間がAIに勝っていることというのは、全て人間というターゲットを包含していて自己参照的です。もはや人間の優位性の立証に関する先の文章の「人間」と「AI」という言葉を入れ替えても成立します。そうしたら人間の優位性は非常に残念ですが無いことになってしまいました。

この(僕的な)結論を裏付けしていきます。

そもそも人間とAIのメカニズム上の違いって何でしょう？ニューラルネットワークは脳のニューロンを真似たものであると説明しましたがそれと同じように、科学的に考えると脳と全く同じ挙動を示すAIを作ることは可能です。極端な方法としては脳の物理的なシミュレーションを行えばいいのです。（もちろん現代のコンピュータではできませんが極端な例としてはわかりやすいですし可能であることが自明です。）

そうしたら人間とAIって区別できません。図らずもAIとは何かの定義にチューリング・テストというものがあります。

>We now ask the question, "What will happen when a machine takes the part of A in this game?" Will the interrogator decide wrongly as often when the game is played like this as he does when the game is played between a man and a woman? These questions replace our original, "Can machines think?"
>引用：COMPUTING MACHINERY AND INTELLIGENCE (A. M. Turing)

>脱線ですがチューリングさんは計算機科学という分野の研究者の方で「コンピュータ科学の父」や「人工知能の父」と呼ばれています。僕の推しの一人です。以上蛇足でした。

人間は何をすればいいかの私的結論は、「AIと人間が実は変わらないということを受け止め絶望しながらもAIを使役して民主主義的効率化を図りAIに奪われないような動的な仕事をすればいい」というものです。

長々と自論を語ってしまいましたが、以上です。
他の記事も読んでね。

# 参考

<!--stackedit_data:
eyJoaXN0b3J5IjpbNTM0NzIwOTgxLDY4MTg1MTE0MywxNjc1MD
YwMTU2LC05NzMzMzA4MjEsMTAzMzY5MDY3NCwxMjMzMjM0OTYw
LC0xMzY2MjY5MjY5LC0xNTQ0Mjg2MjM5LDQwMTIyNjUxN119
-->